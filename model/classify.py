# model/classify.py - CUDA ì¶©ëŒ í•´ê²° ë²„ì „
import torch
import os
import pickle
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from config import MODEL_PATH  # ì˜ˆ: "/home/.../results3"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ ìºì‹œ (ì§€ì—° ë¡œë”©ìš©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_model_cache = None
_tokenizer_cache = None
_label_encoder_cache = None
_device_cache = None

def get_classify_components():
    """ëª¨ë¸, í† í¬ë‚˜ì´ì €, ë¼ë²¨ ì¸ì½”ë”ë¥¼ ì§€ì—° ë¡œë”©ìœ¼ë¡œ ë°˜í™˜"""
    global _model_cache, _tokenizer_cache, _label_encoder_cache, _device_cache
    
    if _model_cache is None:
        print("ğŸ”„ BERT ë¶„ë¥˜ ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # 1) ë””ë°”ì´ìŠ¤ ê²°ì •
        _device_cache = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ’» BERT classify device: {_device_cache}")
        
        # 2) í† í¬ë‚˜ì´ì € ë¡œë“œ
        _tokenizer_cache = AutoTokenizer.from_pretrained(
            MODEL_PATH, 
            local_files_only=True
        )
        
        # 3) ëª¨ë¸ ë¡œë“œ
        _model_cache = (
            AutoModelForSequenceClassification
            .from_pretrained(MODEL_PATH, local_files_only=True)
            .eval()
            .to(_device_cache)
        )
        
        # 4) ë¼ë²¨ ì¸ì½”ë” ë¡œë“œ
        with open(os.path.join(MODEL_PATH, "label_encoder.pkl"), "rb") as f:
            _label_encoder_cache = pickle.load(f)
            
        print("âœ… BERT ë¶„ë¥˜ ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    return _model_cache, _tokenizer_cache, _label_encoder_cache, _device_cache

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¶„ë¥˜ í•¨ìˆ˜ (ì§€ì—° ë¡œë”© ì ìš©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def predict_category(text: str) -> str:
    if not isinstance(text, str) or not text.strip():
        return "ê¸°íƒ€"
    
    try:
        # ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ ê°€ì ¸ì˜¤ê¸° (ì²« í˜¸ì¶œì‹œì—ë§Œ ë¡œë”©)
        model, tokenizer, label_encoder, device = get_classify_components()
        
        # í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=256
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # ì¶”ë¡ 
        with torch.no_grad():
            logits = model(**inputs).logits
        
        pred_id = torch.argmax(logits, 1).item()
        category = label_encoder.inverse_transform([pred_id])[0]
        
        return category
        
    except Exception as e:
        print(f"âŒ BERT ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ê¸°íƒ€"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„ íƒì‚¬í•­: CPU ì „ìš© ëª¨ë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def predict_category_cpu_only(text: str) -> str:
    """CPU ì „ìš©ìœ¼ë¡œ ë¶„ë¥˜ (CUDA ë¬¸ì œ ì™„ì „ íšŒí”¼)"""
    global _model_cache, _tokenizer_cache, _label_encoder_cache
    
    if not isinstance(text, str) or not text.strip():
        return "ê¸°íƒ€"
    
    try:
        if _model_cache is None:
            print("ğŸ”„ BERT ë¶„ë¥˜ ëª¨ë¸ ë¡œë”© ì¤‘ (CPU ì „ìš©)...")
            
            # CPUë¡œ ê°•ì œ ì„¤ì •
            device = torch.device("cpu")
            
            _tokenizer_cache = AutoTokenizer.from_pretrained(
                MODEL_PATH, 
                local_files_only=True
            )
            
            _model_cache = (
                AutoModelForSequenceClassification
                .from_pretrained(MODEL_PATH, local_files_only=True)
                .eval()
                .to(device)  # CPUë¡œ ê°•ì œ
            )
            
            with open(os.path.join(MODEL_PATH, "label_encoder.pkl"), "rb") as f:
                _label_encoder_cache = pickle.load(f)
                
            print("âœ… BERT ë¶„ë¥˜ ëª¨ë¸ ë¡œë”© ì™„ë£Œ (CPU)!")
        
        inputs = _tokenizer_cache(
            text,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=256
        )
        # CPUì´ë¯€ë¡œ .to() í•„ìš” ì—†ìŒ
        
        with torch.no_grad():
            logits = _model_cache(**inputs).logits
        
        pred_id = torch.argmax(logits, 1).item()
        return _label_encoder_cache.inverse_transform([pred_id])[0]
        
    except Exception as e:
        print(f"âŒ BERT ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ê¸°íƒ€"